{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-qg_3VT1Vwk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igG4YIvEIPU8"
      },
      "outputs": [],
      "source": [
        "!pip install ttach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwaARNlS1SoW"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/data/\"\n",
        "!unzip -q \"/content/drive/MyDrive/data/test.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyrGKZd2Mlgd"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/data/\"\n",
        "!unzip -q \"/content/drive/MyDrive/data/train.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVUAPXcG1VH8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json \n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import ttach as tta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nnmFXSwMh7o"
      },
      "outputs": [],
      "source": [
        "csv_files = sorted(glob('/content/drive/MyDrive/data/train/*/*.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnl3ZAHq1lK1"
      },
      "outputs": [],
      "source": [
        "# 분석에 사용할 feature 선택\n",
        "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
        "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
        "\n",
        "csv_files = sorted(glob('/content/drive/MyDrive/data/train/*/*.csv'))\n",
        "\n",
        "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# feature 별 최대값, 최솟값 계산\n",
        "for csv in tqdm(csv_files[1:]):\n",
        "    temp_csv = pd.read_csv(csv)[csv_features]\n",
        "    temp_csv = temp_csv.replace('-',np.nan).dropna()\n",
        "    if len(temp_csv) == 0:\n",
        "        continue\n",
        "    temp_csv = temp_csv.astype(float)\n",
        "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# feature 별 최대값, 최솟값 dictionary 생성\n",
        "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "csv_feature_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsMfKxuZ1m4d"
      },
      "outputs": [],
      "source": [
        "# 변수 설명 csv 파일 참조\n",
        "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
        "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
        "risk = {'1':'초기','2':'중기','3':'말기'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsApR1Al1oa8"
      },
      "outputs": [],
      "source": [
        "label_description = {}\n",
        "for key, value in disease.items():\n",
        "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
        "    for disease_code in value:\n",
        "        for risk_code in risk:\n",
        "            label = f'{key}_{disease_code}_{risk_code}'\n",
        "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
        "list(label_description.items())[:10]\n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9QSeH6Q1p6d"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, files, labels=None, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.files = files\n",
        "        self.csv_feature_dict = csv_feature_dict\n",
        "        self.csv_feature_check = [0]*len(self.files)\n",
        "        self.csv_features = [None]*len(self.files)\n",
        "        self.max_len = 24 * 6\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        file = self.files[i]\n",
        "        file_name = file.split('/')[-1]\n",
        "        \n",
        "        # csv\n",
        "        if self.csv_feature_check[i] == 0:\n",
        "            csv_path = f'{file}/{file_name}.csv'\n",
        "            df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
        "            df = df.replace('-', 0)\n",
        "            # MinMax scaling\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
        "                df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
        "            # zero padding\n",
        "            pad = np.zeros((self.max_len, len(df.columns)))\n",
        "            length = min(self.max_len, len(df))\n",
        "            pad[-length:] = df.to_numpy()[-length:]\n",
        "            # transpose to sequential data\n",
        "            csv_feature = pad.T\n",
        "            self.csv_features[i] = csv_feature\n",
        "            self.csv_feature_check[i] = 1\n",
        "        else:\n",
        "            csv_feature = self.csv_features[i]\n",
        "        \n",
        "        # image\n",
        "        image_path = f'{file}/{file_name}.jpg'\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255\n",
        "        img = np.transpose(img, (2,0,1))\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            json_path = f'{file}/{file_name}.json'\n",
        "            with open(json_path, 'r') as f:\n",
        "                json_file = json.load(f)\n",
        "            \n",
        "            crop = json_file['annotations']['crop']\n",
        "            disease = json_file['annotations']['disease']\n",
        "            risk = json_file['annotations']['risk']\n",
        "            label = f'{crop}_{disease}_{risk}'\n",
        "            \n",
        "            return {\n",
        "                'img' : torch.tensor(img, dtype=torch.float32),\n",
        "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32),\n",
        "                'label' : torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'img' : torch.tensor(img, dtype=torch.float32),\n",
        "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7OKSI4y1toc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "batch_size = 8\n",
        "class_n = len(label_encoder)\n",
        "learning_rate = 1e-4\n",
        "embedding_dim = 512\n",
        "num_features = len(csv_feature_dict)\n",
        "max_len = 24*6\n",
        "dropout_rate = 0.1\n",
        "epochs = 10\n",
        "vision_pretrain = True\n",
        "save_path = 'best_model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V9RqEIR1JTe"
      },
      "outputs": [],
      "source": [
        "test = sorted(glob('/content/drive/MyDrive/data/test/*'))\n",
        "test_dataset = CustomDataset(test, mode = 'test')\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wppoClD31yQ3"
      },
      "outputs": [],
      "source": [
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, class_n, rate=0.1):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.model = models.mobilenet_v3_large(pretrained=True)\n",
        "        #self.model = models.densenet201(pretrained=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.model(inputs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4NuMW1D1y7o"
      },
      "outputs": [],
      "source": [
        "class RNN_Decoder(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, num_features, class_n, rate):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        #self.lstm = nn.LSTM(max_len, embedding_dim,num_layers=1)\n",
        "        self.lstm = nn.LSTM(max_len, embedding_dim,num_layers=3,bidirectional=False) #bidirectional 사용, 3층이 가장 최적화\n",
        "\n",
        "        self.rnn_fc = nn.Linear(num_features*embedding_dim, 1000)\n",
        "        self.final_layer = nn.Linear(1000 + 1000, class_n) # resnet out_dim + lstm out_dim\n",
        "        self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, enc_out, dec_inp):\n",
        "        hidden, _ = self.lstm(dec_inp)\n",
        "        hidden = hidden.view(hidden.size(0), -1)\n",
        "        hidden = self.rnn_fc(hidden)\n",
        "        concat = torch.cat([enc_out, hidden], dim=1) # enc_out + hidden \n",
        "        fc_input = concat\n",
        "        output = self.dropout((self.final_layer(fc_input)))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd7NuKUU1z5m"
      },
      "outputs": [],
      "source": [
        "class CNN2RNN(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, num_features, class_n, rate):\n",
        "        super(CNN2RNN, self).__init__()\n",
        "        self.cnn = CNN_Encoder(embedding_dim, rate)\n",
        "        self.rnn = RNN_Decoder(max_len, embedding_dim, num_features, class_n, rate)\n",
        "        \n",
        "    def forward(self, img, seq):\n",
        "        cnn_output = self.cnn(img)\n",
        "        output = self.rnn(cnn_output, seq)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_2wf5wEN5Vf"
      },
      "outputs": [],
      "source": [
        "model = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, class_n=class_n, rate=dropout_rate)\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "\n",
        "best_models = []\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/data/models/mobilenet_v3_large adamw scheduler 1e-3.pt', map_location=device), strict=False) #56\n",
        "best_models.append(copy.deepcopy(model)) \n",
        "# key success 뜨는지 꼭 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply_tta_model=[]\n",
        "\n",
        "tta_transforms = tta.Compose(\n",
        "    [\n",
        "        tta.Rotate90(angles=[0, 90, 180, 270]),\n",
        "        tta.Multiply([0.9, 1]),\n",
        "    ]\n",
        ")\n",
        "for model in best_models:\n",
        "   apply_tta_model.append(copy.deepcopy(tta.ClassificationTTAWrapper(model, tta_transforms)))"
      ],
      "metadata": {
        "id": "MiVXk1WVEfSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8sOCDZD15OB"
      },
      "outputs": [],
      "source": [
        "def accuracy_function(real, pred):    \n",
        "    real = real.cpu()\n",
        "    pred = torch.argmax(pred, dim=1).cpu()\n",
        "    score = f1_score(real, pred, average='macro')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n5RfJgvv17CN"
      },
      "outputs": [],
      "source": [
        "def predict(dataset):\n",
        "    tqdm_dataset = tqdm(enumerate(dataset))\n",
        "    results = []\n",
        "    for batch, batch_item in tqdm_dataset:\n",
        "        pred=0\n",
        "\n",
        "        img = batch_item['img'].to(device)\n",
        "        seq = batch_item['csv_feature'].to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for model in best_models:\n",
        "              model.eval()\n",
        "              pred += model(img,seq)\n",
        "        output = torch.tensor(torch.argmax(pred, dim=1), dtype=torch.int32).cpu().numpy()\n",
        "        results.extend(output)\n",
        "    return results\n",
        "\n",
        "preds = predict(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQsk87y8173e"
      },
      "outputs": [],
      "source": [
        "preds = np.array([label_decoder[int(val)] for val in preds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JMM_MEHI18gV"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv')\n",
        "submission['label'] = preds\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cWVGdX2N181g"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('/content/drive/MyDrive/data/baseline_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "inference.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}